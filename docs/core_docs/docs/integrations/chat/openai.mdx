---
sidebar_label: OpenAI
---

import CodeBlock from "@theme/CodeBlock";

# ChatOpenAI

You can use OpenAI's chat models as follows:

import OpenAI from "@examples/models/chat/integration_openai.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai
```

import UnifiedModelParamsTooltip from "@mdx_components/unified_model_params_tooltip.mdx";

<UnifiedModelParamsTooltip></UnifiedModelParamsTooltip>

<CodeBlock language="typescript">{OpenAI}</CodeBlock>

If you're part of an organization, you can set `process.env.OPENAI_ORGANIZATION` with your OpenAI organization id, or pass it in as `organization` when
initializing the model.

## Multimodal messages

:::info
This feature is currently in preview. The message schema may change in future releases.
:::

OpenAI supports interleaving images with text in input messages with their `gpt-4-vision-preview`. Here's an example of how this looks:

import OpenAIVision from "@examples/models/chat/integration_openai_vision.ts";

<CodeBlock language="typescript">{OpenAIVision}</CodeBlock>

## Tool calling

:::info
This feature is currently only available for `gpt-3.5-turbo-1106` and `gpt-4-1106-preview` models.
:::

More recent OpenAI chat models support calling multiple functions to get all required data to answer a question.
Here's an example how a conversation turn with this functionality might look:

import OpenAITools from "@examples/models/chat/integration_openai_tool_calls.ts";

<CodeBlock language="typescript">{OpenAITools}</CodeBlock>

### `.withStructuredOutput({ ... })`

:::info
The `.withStructuredOutput` method is in beta. It is actively being worked on, so the API may change.
:::

You can also use the `.withStructuredOutput({ ... })` method to coerce `ChatOpenAI` into returning a structured output.

The method allows for passing in either a Zod object, or a valid JSON schema (like what is returned from [`zodToJsonSchema`](https://www.npmjs.com/package/zod-to-json-schema)).

Using the method is simple. Just define your LLM and call `.withStructuredOutput({ ... })` on it, passing the desired schema.

Here is an example using a Zod schema and the `functionCalling` mode (default mode):

import WSAZodExample from "@examples/models/chat/integration_openai_wsa_zod.ts";

<CodeBlock language="typescript">{WSAZodExample}</CodeBlock>

Additionally, you can pass in an OpenAI function definition or JSON schema directly:

:::info
If using `jsonMode` as the `method` you must include context in your prompt about the structured output you want. This _must_ include the keyword: `JSON`.
:::

import WSAJSONSchemaExample from "@examples/models/chat/integration_openai_wsa_json_schema.ts";

<CodeBlock language="typescript">{WSAJSONSchemaExample}</CodeBlock>

## Custom URLs

You can customize the base URL the SDK sends requests to by passing a `configuration` parameter like this:

import OpenAICustomBase from "@examples/models/chat/integration_openai_custom_base.ts";

<CodeBlock language="typescript">{OpenAICustomBase}</CodeBlock>

You can also pass other `ClientOptions` parameters accepted by the official SDK.

If you are hosting on Azure OpenAI, see the [dedicated page instead](/docs/integrations/chat/azure).

## Calling fine-tuned models

You can call fine-tuned OpenAI models by passing in your corresponding `modelName` parameter.

This generally takes the form of `ft:{OPENAI_MODEL_NAME}:{ORG_NAME}::{MODEL_ID}`. For example:

import OpenAIFineTuned from "@examples/models/chat/integration_openai_fine_tune.ts";

<CodeBlock language="typescript">{OpenAIFineTuned}</CodeBlock>

## Generation metadata

If you need additional information like logprobs or token usage, these will be returned directly in the `.invoke` response.

:::tip
Requires `@langchain/core` version >=0.1.48.
:::

import OpenAIInvokeInfo from "@examples/models/chat/integration_openai_invoke_info.ts";
import OpenAIGenerationInfo from "@examples/models/chat/integration_openai_generation_info.ts";
import OpenAICallbacks from "@examples/models/chat/integration_openai_callbacks.ts";

<CodeBlock language="typescript">{OpenAIInvokeInfo}</CodeBlock>

### With callbacks

You can also use the callbacks system:

<CodeBlock language="typescript">{OpenAICallbacks}</CodeBlock>

### With `.generate()`

<CodeBlock language="typescript">{OpenAIGenerationInfo}</CodeBlock>

### Streaming tokens

OpenAI supports streaming token counts via an opt-in call option. This can be set by passing `{ stream_options: { include_usage: true } }`.
Setting this call option will cause the model to return an additional chunk at the end of the stream, containing the token usage.

import OpenAIStreamTokens from "@examples/models/chat/integration_openai_stream_tokens.ts";

<CodeBlock language="typescript">{OpenAIStreamTokens}</CodeBlock>

:::tip
See the LangSmith trace [here](https://smith.langchain.com/public/66bf7377-cc69-4676-91b6-25929a05e8b7/r)
:::

### Disabling parallel tool calls

If you have multiple tools bound to the model, but you'd only like for a single tool to be called at a time, you can pass the `parallel_tool_calls` call option to enable/disable this behavior.
By default, `parallel_tool_calls` is set to `true`.

import OpenAIParallelToolCallsTokens from "@examples/models/chat/integration_openai_parallel_tool_calls.ts";

<CodeBlock language="typescript">{OpenAIParallelToolCallsTokens}</CodeBlock>

:::tip
See the LangSmith trace for the first invocation [here](https://smith.langchain.com/public/68f2ff13-6331-47d8-a8c0-d1745788e84e/r) and the second invocation [here](https://smith.langchain.com/public/6c2fff29-9470-486a-8715-805fda631024/r)
:::
